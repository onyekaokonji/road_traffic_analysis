{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1pW8QG8BBobcWNJmAlH6ZPW8IVeeJTyLN",
      "authorship_tag": "ABX9TyO4tHhfB0aD2RvnSpa+QCh7"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "be3de80b33f1409a8359973a8cee856a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cc06ddfffafc4a92a0ccfb366436d839",
              "IPY_MODEL_9b48d3cad00a46f99991325a7ab07aa2",
              "IPY_MODEL_d8841c23f20e4cbcac0d1d58af8aaf78"
            ],
            "layout": "IPY_MODEL_6e2b7cd35bb54e00a3415159b2078aae"
          }
        },
        "cc06ddfffafc4a92a0ccfb366436d839": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6248f07384448bf8d7a4ad439ced5ab",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_95cd1211976547fc837a935199242c08",
            "value": "100%"
          }
        },
        "9b48d3cad00a46f99991325a7ab07aa2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c99fef8d6684d30a64a1f49577f663f",
            "max": 87769683,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1f0dbb4f16844151b792a50b10efdf79",
            "value": 87769683
          }
        },
        "d8841c23f20e4cbcac0d1d58af8aaf78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99d54b499b314163b1208a63a89b7398",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_1f35a7fe9b58486cbdd22b29dd30a3d4",
            "value": " 83.7M/83.7M [00:01&lt;00:00, 71.8MB/s]"
          }
        },
        "6e2b7cd35bb54e00a3415159b2078aae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6248f07384448bf8d7a4ad439ced5ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95cd1211976547fc837a935199242c08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1c99fef8d6684d30a64a1f49577f663f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f0dbb4f16844151b792a50b10efdf79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "99d54b499b314163b1208a63a89b7398": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f35a7fe9b58486cbdd22b29dd30a3d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###### imports and installs"
      ],
      "metadata": {
        "id": "olufkHstPz8a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVYYjnpQAI-f",
        "outputId": "d5df8767-c882-4d8f-d955-a641552aa6dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m308.3/308.3 KB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ifzhang/ByteTrack.git\n",
        "!cd ByteTrack && pip3 install -q -r requirements.txt\n",
        "!cd ByteTrack && python3 setup.py -q develop\n",
        "!pip install -q cython_bbox\n",
        "!pip install -q onemetric\n",
        "\n",
        "import sys\n",
        "sys.path.append(f\"/content/ByteTrack\")"
      ],
      "metadata": {
        "id": "8jd6_t8lC869",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1643bedf-0d5a-4b9d-c00e-81a7b0666076"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ByteTrack'...\n",
            "remote: Enumerating objects: 2007, done.\u001b[K\n",
            "remote: Total 2007 (delta 0), reused 0 (delta 0), pack-reused 2007\u001b[K\n",
            "Receiving objects: 100% (2007/2007), 79.60 MiB | 33.72 MiB/s, done.\n",
            "Resolving deltas: 100% (1141/1141), done.\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.3/58.3 KB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m146.0/146.0 KB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m161.5/161.5 KB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m178.0/178.0 KB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.5/14.5 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m647.0/647.0 KB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for onnx-simplifier (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for lap (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for filterpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "package init file 'datasets/__init__.py' not found (or not a regular file)\n",
            "package init file 'tools/__init__.py' not found (or not a regular file)\n",
            "package init file 'tutorials/__init__.py' not found (or not a regular file)\n",
            "package init file 'exps/__init__.py' not found (or not a regular file)\n",
            "package init file 'deploy/__init__.py' not found (or not a regular file)\n",
            "package init file 'videos/__init__.py' not found (or not a regular file)\n",
            "package init file 'assets/__init__.py' not found (or not a regular file)\n",
            "package init file 'datasets/data_path/__init__.py' not found (or not a regular file)\n",
            "package init file 'yolox/tracking_utils/__init__.py' not found (or not a regular file)\n",
            "package init file 'yolox/deepsort_tracker/__init__.py' not found (or not a regular file)\n",
            "package init file 'yolox/motdt_tracker/__init__.py' not found (or not a regular file)\n",
            "package init file 'yolox/tracker/__init__.py' not found (or not a regular file)\n",
            "package init file 'yolox/sort_tracker/__init__.py' not found (or not a regular file)\n",
            "package init file 'yolox/layers/csrc/__init__.py' not found (or not a regular file)\n",
            "package init file 'yolox/layers/csrc/cocoeval/__init__.py' not found (or not a regular file)\n",
            "package init file 'tutorials/centertrack/__init__.py' not found (or not a regular file)\n",
            "package init file 'tutorials/trades/__init__.py' not found (or not a regular file)\n",
            "package init file 'tutorials/transtrack/__init__.py' not found (or not a regular file)\n",
            "package init file 'tutorials/fairmot/__init__.py' not found (or not a regular file)\n",
            "package init file 'tutorials/cstrack/__init__.py' not found (or not a regular file)\n",
            "package init file 'tutorials/motr/__init__.py' not found (or not a regular file)\n",
            "package init file 'tutorials/jde/__init__.py' not found (or not a regular file)\n",
            "package init file 'tutorials/ctracker/__init__.py' not found (or not a regular file)\n",
            "package init file 'tutorials/qdtrack/__init__.py' not found (or not a regular file)\n",
            "package init file 'tutorials/centertrack/mot_online/__init__.py' not found (or not a regular file)\n",
            "package init file 'tutorials/trades/mot_online/__init__.py' not found (or not a regular file)\n",
            "package init file 'tutorials/transtrack/mot_online/__init__.py' not found (or not a regular file)\n",
            "package init file 'tutorials/motr/mot_online/__init__.py' not found (or not a regular file)\n",
            "package init file 'tutorials/ctracker/mot_online/__init__.py' not found (or not a regular file)\n",
            "package init file 'tutorials/qdtrack/mot_online/__init__.py' not found (or not a regular file)\n",
            "package init file 'exps/example/__init__.py' not found (or not a regular file)\n",
            "package init file 'exps/default/__init__.py' not found (or not a regular file)\n",
            "package init file 'exps/example/mot/__init__.py' not found (or not a regular file)\n",
            "package init file 'deploy/ncnn/__init__.py' not found (or not a regular file)\n",
            "package init file 'deploy/TensorRT/__init__.py' not found (or not a regular file)\n",
            "package init file 'deploy/ONNXRuntime/__init__.py' not found (or not a regular file)\n",
            "package init file 'deploy/DeepStream/__init__.py' not found (or not a regular file)\n",
            "package init file 'deploy/ncnn/cpp/__init__.py' not found (or not a regular file)\n",
            "package init file 'deploy/ncnn/cpp/src/__init__.py' not found (or not a regular file)\n",
            "package init file 'deploy/ncnn/cpp/include/__init__.py' not found (or not a regular file)\n",
            "package init file 'deploy/TensorRT/python/__init__.py' not found (or not a regular file)\n",
            "package init file 'deploy/TensorRT/cpp/__init__.py' not found (or not a regular file)\n",
            "package init file 'deploy/TensorRT/cpp/src/__init__.py' not found (or not a regular file)\n",
            "package init file 'deploy/TensorRT/cpp/include/__init__.py' not found (or not a regular file)\n",
            "package init file 'deploy/DeepStream/images/__init__.py' not found (or not a regular file)\n",
            "package init file 'deploy/DeepStream/src/__init__.py' not found (or not a regular file)\n",
            "package init file 'deploy/DeepStream/includes/__init__.py' not found (or not a regular file)\n",
            "package init file 'deploy/DeepStream/cmake/__init__.py' not found (or not a regular file)\n",
            "Emitting ninja build file /content/ByteTrack/build/temp.linux-x86_64-3.8/build.ninja...\n",
            "Compiling objects...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "[1/2] c++ -MMD -MF /content/ByteTrack/build/temp.linux-x86_64-3.8/content/ByteTrack/yolox/layers/csrc/cocoeval/cocoeval.o.d -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/content/ByteTrack/yolox/layers/csrc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/include/python3.8 -c -c /content/ByteTrack/yolox/layers/csrc/cocoeval/cocoeval.cpp -o /content/ByteTrack/build/temp.linux-x86_64-3.8/content/ByteTrack/yolox/layers/csrc/cocoeval/cocoeval.o -O3 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "[2/2] c++ -MMD -MF /content/ByteTrack/build/temp.linux-x86_64-3.8/content/ByteTrack/yolox/layers/csrc/vision.o.d -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/content/ByteTrack/yolox/layers/csrc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/include/python3.8 -c -c /content/ByteTrack/yolox/layers/csrc/vision.cpp -o /content/ByteTrack/build/temp.linux-x86_64-3.8/content/ByteTrack/yolox/layers/csrc/vision.o -O3 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m41.3/41.3 KB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for cython_bbox (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install supervision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6FlUC7HDiDB",
        "outputId": "f2fe9b75-b888-4027-f362-eb96eec67046"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting supervision\n",
            "  Downloading supervision-0.3.0-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from supervision) (3.5.3)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.8/dist-packages (from supervision) (4.6.0.66)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.8/dist-packages (from supervision) (1.22.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib->supervision) (2.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->supervision) (23.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->supervision) (4.38.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->supervision) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->supervision) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->supervision) (0.11.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->supervision) (8.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7->matplotlib->supervision) (1.15.0)\n",
            "Installing collected packages: supervision\n",
            "Successfully installed supervision-0.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ultralytics\n",
        "ultralytics.checks()\n",
        "\n",
        "from ultralytics import YOLO\n",
        "\n",
        "import yolox\n",
        "print(\"yolox.__version__:\", yolox.__version__)\n",
        "\n",
        "from yolox.tracker.byte_tracker import BYTETracker, STrack\n",
        "from onemetric.cv.utils.iou import box_iou_batch\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import supervision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbijxcpSCxMc",
        "outputId": "e9ec6391-6433-4c85-850b-1cf35af86279"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ultralytics YOLOv8.0.50 ðŸš€ Python-3.8.10 torch-1.13.1+cu116 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Setup complete âœ… (2 CPUs, 12.7 GB RAM, 25.9/166.8 GB disk)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yolox.__version__: 0.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from supervision.draw.color import ColorPalette, DEFAULT_COLOR_PALETTE\n",
        "from supervision.geometry.core import Point\n",
        "from supervision.video import VideoInfo\n",
        "from supervision.video import get_video_frames_generator\n",
        "from supervision.video import VideoSink\n",
        "from supervision.notebook.utils import show_frame_in_notebook\n",
        "from supervision.detection.core import Detections, BoxAnnotator\n",
        "from supervision.detection.line_counter import LineZone, LineZoneAnnotator\n",
        "from supervision.detection.polygon_zone import PolygonZone, PolygonZoneAnnotator"
      ],
      "metadata": {
        "id": "ypNBa4jjF-n9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### storing BYTETracker arguments"
      ],
      "metadata": {
        "id": "FDOSRe6sP5Sx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass(frozen=True)\n",
        "class BYTETrackerArgs:\n",
        "    track_thresh: float = 0.25\n",
        "    track_buffer: int = 30\n",
        "    match_thresh: float = 0.8\n",
        "    aspect_ratio_thresh: float = 3.0\n",
        "    min_box_area: float = 1.0\n",
        "    mot20: bool = False"
      ],
      "metadata": {
        "id": "rOajZzbyEsFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bytetracker detections need to match those of yolo8 model\n",
        "\n",
        "from typing import List\n",
        "import numpy as np\n",
        "\n",
        "def detection2boxes(detections):\n",
        "  return np.hstack((detections.xyxy, detections.confidence[:, np.newaxis]))\n",
        "\n",
        "\n",
        "def tracks2boxes(tracks):\n",
        "  return np.array([track.tlbr for track in tracks], dtype=float)\n",
        "\n",
        "\n",
        "def match_detections_with_tracks(detections, tracks):\n",
        "  if not np.any(detections.xyxy) or len(tracks) == 0:\n",
        "    return np.empty((0,))\n",
        "  \n",
        "  track_boxes = tracks2boxes(tracks)\n",
        "  iou = box_iou_batch(track_boxes, detections.xyxy)\n",
        "  track2detection = np.argmax(iou, axis=1)\n",
        "\n",
        "  tracker_ids = [None] * len(detections)\n",
        "\n",
        "  for tracker_index, detection_index in enumerate(track2detection):\n",
        "    if iou[tracker_index, detection_index] != 0:\n",
        "      tracker_ids[detection_index] = tracks[tracker_index].track_id\n",
        "\n",
        "  return tracker_ids"
      ],
      "metadata": {
        "id": "G6IPv75GGSUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### main"
      ],
      "metadata": {
        "id": "AoEZ8LTNQGq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = YOLO('yolov8l.pt')\n",
        "model.fuse()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "be3de80b33f1409a8359973a8cee856a",
            "cc06ddfffafc4a92a0ccfb366436d839",
            "9b48d3cad00a46f99991325a7ab07aa2",
            "d8841c23f20e4cbcac0d1d58af8aaf78",
            "6e2b7cd35bb54e00a3415159b2078aae",
            "e6248f07384448bf8d7a4ad439ced5ab",
            "95cd1211976547fc837a935199242c08",
            "1c99fef8d6684d30a64a1f49577f663f",
            "1f0dbb4f16844151b792a50b10efdf79",
            "99d54b499b314163b1208a63a89b7398",
            "1f35a7fe9b58486cbdd22b29dd30a3d4"
          ]
        },
        "id": "QVIo1H2uKb8e",
        "outputId": "a4a9f0b7-945c-4130-a547-52d625cf5556"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8l.pt to yolov8l.pt...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/83.7M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "be3de80b33f1409a8359973a8cee856a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv8l summary (fused): 268 layers, 43668288 parameters, 0 gradients, 165.2 GFLOPs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = \"video_path\" # input video \n",
        "video_info = VideoInfo.from_video_path(video_path) #Video Info object containing dimensions and number of frame in video\n",
        "result_path = \"result.mp4\" # resulting video output\n",
        "interested_class_ids = [0, 3, 5, 7] # classes we are interested in detecting and tracking"
      ],
      "metadata": {
        "id": "xDhKrHSxx4Re"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extracting the start and end points of the line over which objects must cross to be counted\n",
        "a = Point(5, int(video_info.height // 1.7)) \n",
        "b = Point(int(video_info.width - 5), int(video_info.height // 1.7))\n",
        "\n",
        "\n",
        "# obtaining Polygon cordinates - Polygons help determine what area of the frame to conduct\n",
        "# detections and tracking in \n",
        "polygon = np.array([[21, 410],[1257, 414],[1261, 682],[13, 682]])"
      ],
      "metadata": {
        "id": "6VsQciwDSDO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initializing necessary objects for tracking, zoning and counting\n",
        "\n",
        "colors = ColorPalette.from_hex(DEFAULT_COLOR_PALETTE)\n",
        "byte_tracker = BYTETracker(BYTETrackerArgs)\n",
        "box_annotator = BoxAnnotator()\n",
        "line_zone = LineZone(start = a, end = b)\n",
        "line_zone_annotator = LineZoneAnnotator()\n",
        "zone = PolygonZone(polygon, frame_resolution_wh=(video_info.width, video_info.height))\n",
        "zone_annotator = PolygonZoneAnnotator(zone, color = colors.by_idx(1))"
      ],
      "metadata": {
        "id": "FZeSAv42Ti2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating video output while conducting detection, tracking and counting\n",
        "\n",
        "with VideoSink(result_path, video_info) as video:\n",
        "  \n",
        "  for frame in get_video_frames_generator(video_path):\n",
        "\n",
        "    result = model.predict(frame)\n",
        "\n",
        "    detections = Detections.from_yolov8(result[0])\n",
        "\n",
        "    mask = np.array([id in interested_class_ids for id in detections.class_id], dtype = bool)\n",
        "\n",
        "    detections.filter(mask, inplace=True)\n",
        "\n",
        "    tracks = byte_tracker.update(output_results = detection2boxes(detections), img_info = frame.shape, img_size = frame.shape)\n",
        "\n",
        "    track_ids = match_detections_with_tracks(detections, tracks)\n",
        "\n",
        "    detections.tracker_id = np.array(track_ids)\n",
        "\n",
        "    second_mask = zone.trigger(detections)\n",
        "    \n",
        "    detections = detections[second_mask]\n",
        "\n",
        "    labels = [f\"{tracker_id}-{model.names[class_id]}\" for _, _, class_id, tracker_id in detections]\n",
        "\n",
        "    scene = box_annotator.annotate(scene=frame, detections=detections, labels=labels)\n",
        "\n",
        "    line_zone_annotator.annotate(frame = scene, line_counter = line_zone)\n",
        "\n",
        "    line_zone.trigger(detections)\n",
        "\n",
        "    zone_annotator.annotate(scene)\n",
        "\n",
        "    video.write_frame(scene)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moH_TZ7evNwu",
        "outputId": "d0288bd5-ad42-462e-8bc6-e4ba75c06522"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0: 384x640 6 cars, 3 buss, 3 trucks, 1 bench, 41.8ms\n",
            "Speed: 1.0ms preprocess, 41.8ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 6 cars, 3 buss, 3 trucks, 1 bench, 41.1ms\n",
            "Speed: 0.7ms preprocess, 41.1ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 7 cars, 2 buss, 3 trucks, 1 traffic light, 41.2ms\n",
            "Speed: 0.5ms preprocess, 41.2ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 6 cars, 1 bus, 4 trucks, 41.2ms\n",
            "Speed: 0.6ms preprocess, 41.2ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1 bus, 4 trucks, 28.4ms\n",
            "Speed: 1.0ms preprocess, 28.4ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 5 cars, 1 bus, 4 trucks, 29.6ms\n",
            "Speed: 0.7ms preprocess, 29.6ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 5 cars, 2 buss, 2 trucks, 28.4ms\n",
            "Speed: 0.6ms preprocess, 28.4ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 5 cars, 1 bus, 3 trucks, 27.1ms\n",
            "Speed: 0.5ms preprocess, 27.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 5 cars, 4 trucks, 25.6ms\n",
            "Speed: 1.0ms preprocess, 25.6ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 5 cars, 4 trucks, 25.1ms\n",
            "Speed: 0.5ms preprocess, 25.1ms inference, 5.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 5 cars, 1 bus, 4 trucks, 25.3ms\n",
            "Speed: 0.6ms preprocess, 25.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 4 cars, 1 bus, 4 trucks, 24.4ms\n",
            "Speed: 0.6ms preprocess, 24.4ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 4 cars, 1 bus, 3 trucks, 22.5ms\n",
            "Speed: 0.7ms preprocess, 22.5ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 5 cars, 1 bus, 3 trucks, 1 traffic light, 21.9ms\n",
            "Speed: 0.6ms preprocess, 21.9ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 4 cars, 1 bus, 3 trucks, 2 traffic lights, 22.6ms\n",
            "Speed: 0.7ms preprocess, 22.6ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 4 cars, 1 bus, 3 trucks, 20.5ms\n",
            "Speed: 0.9ms preprocess, 20.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 6 cars, 1 bus, 4 trucks, 1 bench, 20.9ms\n",
            "Speed: 0.6ms preprocess, 20.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1 bus, 3 trucks, 1 traffic light, 26.8ms\n",
            "Speed: 0.6ms preprocess, 26.8ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 5 cars, 1 bus, 4 trucks, 1 traffic light, 20.7ms\n",
            "Speed: 0.6ms preprocess, 20.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 5 cars, 1 bus, 4 trucks, 1 traffic light, 20.4ms\n",
            "Speed: 0.5ms preprocess, 20.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 4 cars, 1 bus, 4 trucks, 1 traffic light, 20.4ms\n",
            "Speed: 0.7ms preprocess, 20.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 5 cars, 5 trucks, 1 traffic light, 2 benchs, 20.4ms\n",
            "Speed: 1.0ms preprocess, 20.4ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 5 cars, 1 bus, 6 trucks, 1 traffic light, 2 benchs, 20.4ms\n",
            "Speed: 1.3ms preprocess, 20.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 5 cars, 1 bus, 6 trucks, 1 traffic light, 1 bench, 20.5ms\n",
            "Speed: 0.5ms preprocess, 20.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 6 cars, 5 trucks, 1 traffic light, 20.6ms\n",
            "Speed: 1.0ms preprocess, 20.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 7 cars, 5 trucks, 2 traffic lights, 20.6ms\n",
            "Speed: 1.1ms preprocess, 20.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 7 cars, 5 trucks, 1 bench, 20.6ms\n",
            "Speed: 0.5ms preprocess, 20.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 6 cars, 1 bus, 5 trucks, 1 traffic light, 20.5ms\n",
            "Speed: 0.8ms preprocess, 20.5ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 6 cars, 5 trucks, 1 traffic light, 20.7ms\n",
            "Speed: 0.7ms preprocess, 20.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 5 cars, 2 buss, 5 trucks, 20.2ms\n",
            "Speed: 1.7ms preprocess, 20.2ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 5 cars, 2 buss, 3 trucks, 1 traffic light, 20.2ms\n",
            "Speed: 0.6ms preprocess, 20.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 4 cars, 2 buss, 4 trucks, 19.8ms\n",
            "Speed: 0.5ms preprocess, 19.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 6 cars, 2 buss, 3 trucks, 20.1ms\n",
            "Speed: 0.6ms preprocess, 20.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 5 cars, 2 buss, 4 trucks, 19.7ms\n",
            "Speed: 0.6ms preprocess, 19.7ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 5 cars, 2 buss, 4 trucks, 20.2ms\n",
            "Speed: 0.9ms preprocess, 20.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 5 cars, 1 bus, 4 trucks, 19.6ms\n",
            "Speed: 0.6ms preprocess, 19.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 6 cars, 4 trucks, 20.3ms\n",
            "Speed: 0.9ms preprocess, 20.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 7 cars, 4 trucks, 19.4ms\n",
            "Speed: 0.5ms preprocess, 19.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 6 cars, 1 bus, 3 trucks, 29.0ms\n",
            "Speed: 0.7ms preprocess, 29.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 6 cars, 3 trucks, 21.7ms\n",
            "Speed: 0.6ms preprocess, 21.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 6 cars, 4 trucks, 19.7ms\n",
            "Speed: 0.5ms preprocess, 19.7ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 7 cars, 3 trucks, 1 traffic light, 19.9ms\n",
            "Speed: 0.6ms preprocess, 19.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 7 cars, 4 trucks, 19.6ms\n",
            "Speed: 0.7ms preprocess, 19.6ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 7 cars, 4 trucks, 21.3ms\n",
            "Speed: 0.6ms preprocess, 21.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 6 cars, 4 trucks, 21.1ms\n",
            "Speed: 0.6ms preprocess, 21.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 6 cars, 1 bus, 4 trucks, 20.1ms\n",
            "Speed: 0.5ms preprocess, 20.1ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1 bus, 4 trucks, 19.4ms\n",
            "Speed: 1.1ms preprocess, 19.4ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1 bus, 4 trucks, 21.6ms\n",
            "Speed: 0.6ms preprocess, 21.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1 bus, 4 trucks, 20.1ms\n",
            "Speed: 0.9ms preprocess, 20.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1 bus, 4 trucks, 1 traffic light, 19.4ms\n",
            "Speed: 0.5ms preprocess, 19.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1 bus, 4 trucks, 21.4ms\n",
            "Speed: 0.7ms preprocess, 21.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1 bus, 3 trucks, 19.7ms\n",
            "Speed: 0.6ms preprocess, 19.7ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1 bus, 3 trucks, 20.1ms\n",
            "Speed: 0.7ms preprocess, 20.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1 bus, 3 trucks, 22.5ms\n",
            "Speed: 0.6ms preprocess, 22.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1 bus, 2 trucks, 20.9ms\n",
            "Speed: 1.0ms preprocess, 20.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1 bus, 3 trucks, 19.4ms\n",
            "Speed: 0.6ms preprocess, 19.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1 bus, 3 trucks, 19.8ms\n",
            "Speed: 0.7ms preprocess, 19.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 8 cars, 3 trucks, 19.7ms\n",
            "Speed: 0.5ms preprocess, 19.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1 bus, 3 trucks, 25.1ms\n",
            "Speed: 0.5ms preprocess, 25.1ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 6 cars, 1 bus, 3 trucks, 21.7ms\n",
            "Speed: 0.6ms preprocess, 21.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 6 cars, 1 bus, 3 trucks, 19.4ms\n",
            "Speed: 0.5ms preprocess, 19.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 6 cars, 1 bus, 3 trucks, 20.0ms\n",
            "Speed: 0.6ms preprocess, 20.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 6 cars, 1 bus, 3 trucks, 20.0ms\n",
            "Speed: 0.7ms preprocess, 20.0ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 6 cars, 5 trucks, 19.8ms\n",
            "Speed: 0.6ms preprocess, 19.8ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 5 cars, 1 bus, 5 trucks, 20.0ms\n",
            "Speed: 0.5ms preprocess, 20.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 5 cars, 1 bus, 3 trucks, 19.4ms\n",
            "Speed: 1.3ms preprocess, 19.4ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 5 cars, 3 trucks, 19.7ms\n",
            "Speed: 0.6ms preprocess, 19.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 5 cars, 3 trucks, 20.0ms\n",
            "Speed: 0.6ms preprocess, 20.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 5 cars, 3 trucks, 20.4ms\n",
            "Speed: 0.6ms preprocess, 20.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 5 cars, 1 bus, 3 trucks, 20.1ms\n",
            "Speed: 0.6ms preprocess, 20.1ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 6 cars, 1 bus, 4 trucks, 24.3ms\n",
            "Speed: 0.7ms preprocess, 24.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 5 cars, 3 trucks, 19.8ms\n",
            "Speed: 0.6ms preprocess, 19.8ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 6 cars, 3 trucks, 20.1ms\n",
            "Speed: 0.7ms preprocess, 20.1ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1 bus, 3 trucks, 19.2ms\n",
            "Speed: 2.4ms preprocess, 19.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1 bus, 3 trucks, 21.1ms\n",
            "Speed: 0.6ms preprocess, 21.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 7 cars, 1 bus, 3 trucks, 19.9ms\n",
            "Speed: 0.6ms preprocess, 19.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 7 cars, 3 trucks, 1 traffic light, 18.9ms\n",
            "Speed: 0.5ms preprocess, 18.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 7 cars, 3 trucks, 1 traffic light, 20.5ms\n",
            "Speed: 0.6ms preprocess, 20.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 7 cars, 3 trucks, 1 traffic light, 19.2ms\n",
            "Speed: 0.6ms preprocess, 19.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 8 cars, 3 trucks, 19.5ms\n",
            "Speed: 0.5ms preprocess, 19.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 8 cars, 3 trucks, 27.6ms\n",
            "Speed: 0.6ms preprocess, 27.6ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 7 cars, 3 trucks, 1 traffic light, 19.7ms\n",
            "Speed: 0.6ms preprocess, 19.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 7 cars, 3 trucks, 1 traffic light, 19.8ms\n",
            "Speed: 0.6ms preprocess, 19.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 8 cars, 4 trucks, 1 traffic light, 19.4ms\n",
            "Speed: 0.6ms preprocess, 19.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 7 cars, 4 trucks, 1 traffic light, 19.1ms\n",
            "Speed: 0.6ms preprocess, 19.1ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 6 cars, 4 trucks, 1 traffic light, 19.7ms\n",
            "Speed: 0.5ms preprocess, 19.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 8 cars, 5 trucks, 1 traffic light, 23.1ms\n",
            "Speed: 0.6ms preprocess, 23.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 6 cars, 4 trucks, 1 traffic light, 19.8ms\n",
            "Speed: 0.9ms preprocess, 19.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 6 cars, 4 trucks, 1 traffic light, 19.7ms\n",
            "Speed: 0.8ms preprocess, 19.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 6 cars, 4 trucks, 1 traffic light, 18.9ms\n",
            "Speed: 0.5ms preprocess, 18.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 7 cars, 4 trucks, 1 traffic light, 21.2ms\n",
            "Speed: 0.6ms preprocess, 21.2ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 7 cars, 3 trucks, 1 traffic light, 20.0ms\n",
            "Speed: 0.6ms preprocess, 20.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 6 cars, 4 trucks, 1 traffic light, 19.0ms\n",
            "Speed: 0.5ms preprocess, 19.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 6 cars, 4 trucks, 1 traffic light, 19.8ms\n",
            "Speed: 0.7ms preprocess, 19.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 6 cars, 4 trucks, 1 traffic light, 51.6ms\n",
            "Speed: 0.5ms preprocess, 51.6ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 7 cars, 4 trucks, 1 traffic light, 61.2ms\n",
            "Speed: 6.8ms preprocess, 61.2ms inference, 9.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 7 cars, 4 trucks, 1 traffic light, 33.4ms\n",
            "Speed: 0.6ms preprocess, 33.4ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 6 cars, 4 trucks, 1 traffic light, 19.2ms\n",
            "Speed: 0.6ms preprocess, 19.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 6 cars, 3 trucks, 1 traffic light, 24.3ms\n",
            "Speed: 0.5ms preprocess, 24.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 7 cars, 3 trucks, 1 traffic light, 19.5ms\n",
            "Speed: 0.7ms preprocess, 19.5ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 6 cars, 3 trucks, 1 traffic light, 19.8ms\n",
            "Speed: 0.6ms preprocess, 19.8ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 8 cars, 2 trucks, 1 traffic light, 20.4ms\n",
            "Speed: 0.6ms preprocess, 20.4ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 8 cars, 2 trucks, 1 traffic light, 20.4ms\n",
            "Speed: 1.0ms preprocess, 20.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 8 cars, 2 trucks, 19.4ms\n",
            "Speed: 0.8ms preprocess, 19.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 7 cars, 3 trucks, 20.1ms\n",
            "Speed: 0.5ms preprocess, 20.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 7 cars, 2 trucks, 1 traffic light, 20.1ms\n",
            "Speed: 0.6ms preprocess, 20.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 8 cars, 2 trucks, 1 traffic light, 20.1ms\n",
            "Speed: 0.6ms preprocess, 20.1ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 6 cars, 4 trucks, 1 traffic light, 19.3ms\n",
            "Speed: 1.4ms preprocess, 19.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 6 cars, 4 trucks, 1 traffic light, 117.5ms\n",
            "Speed: 0.6ms preprocess, 117.5ms inference, 11.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 7 cars, 3 trucks, 1 traffic light, 42.4ms\n",
            "Speed: 0.6ms preprocess, 42.4ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 7 cars, 2 trucks, 1 traffic light, 25.1ms\n",
            "Speed: 0.7ms preprocess, 25.1ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 8 cars, 3 trucks, 1 traffic light, 19.9ms\n",
            "Speed: 0.6ms preprocess, 19.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 8 cars, 3 trucks, 1 traffic light, 21.6ms\n",
            "Speed: 0.5ms preprocess, 21.6ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 10 cars, 2 trucks, 1 traffic light, 21.9ms\n",
            "Speed: 0.7ms preprocess, 21.9ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 10 cars, 4 trucks, 1 traffic light, 25.2ms\n",
            "Speed: 0.6ms preprocess, 25.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 11 cars, 3 trucks, 1 traffic light, 19.1ms\n",
            "Speed: 0.6ms preprocess, 19.1ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 10 cars, 4 trucks, 1 traffic light, 20.3ms\n",
            "Speed: 0.6ms preprocess, 20.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 9 cars, 4 trucks, 1 traffic light, 19.3ms\n",
            "Speed: 0.5ms preprocess, 19.3ms inference, 3.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 10 cars, 3 trucks, 1 traffic light, 20.4ms\n",
            "Speed: 0.6ms preprocess, 20.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 9 cars, 3 trucks, 1 traffic light, 19.8ms\n",
            "Speed: 0.6ms preprocess, 19.8ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 11 cars, 4 trucks, 1 traffic light, 20.3ms\n",
            "Speed: 0.6ms preprocess, 20.3ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 11 cars, 5 trucks, 1 traffic light, 88.2ms\n",
            "Speed: 0.6ms preprocess, 88.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 10 cars, 4 trucks, 1 traffic light, 25.5ms\n",
            "Speed: 0.7ms preprocess, 25.5ms inference, 13.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 11 cars, 4 trucks, 1 traffic light, 19.4ms\n",
            "Speed: 1.4ms preprocess, 19.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 10 cars, 3 trucks, 19.4ms\n",
            "Speed: 0.5ms preprocess, 19.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 11 cars, 4 trucks, 20.3ms\n",
            "Speed: 0.6ms preprocess, 20.3ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 12 cars, 3 trucks, 1 traffic light, 19.4ms\n",
            "Speed: 0.5ms preprocess, 19.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 12 cars, 4 trucks, 22.0ms\n",
            "Speed: 0.6ms preprocess, 22.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 11 cars, 3 trucks, 20.2ms\n",
            "Speed: 0.6ms preprocess, 20.2ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 11 cars, 4 trucks, 19.3ms\n",
            "Speed: 0.7ms preprocess, 19.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 10 cars, 4 trucks, 1 traffic light, 25.7ms\n",
            "Speed: 0.7ms preprocess, 25.7ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 11 cars, 3 trucks, 28.8ms\n",
            "Speed: 0.6ms preprocess, 28.8ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 11 cars, 4 trucks, 26.1ms\n",
            "Speed: 0.6ms preprocess, 26.1ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 11 cars, 4 trucks, 38.5ms\n",
            "Speed: 0.6ms preprocess, 38.5ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 11 cars, 5 trucks, 24.8ms\n",
            "Speed: 1.0ms preprocess, 24.8ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 9 cars, 4 trucks, 22.8ms\n",
            "Speed: 0.6ms preprocess, 22.8ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1 bus, 2 trucks, 27.4ms\n",
            "Speed: 0.6ms preprocess, 27.4ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1 bus, 2 trucks, 22.2ms\n",
            "Speed: 0.6ms preprocess, 22.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1 bus, 2 trucks, 26.5ms\n",
            "Speed: 0.6ms preprocess, 26.5ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1 bus, 3 trucks, 21.1ms\n",
            "Speed: 0.6ms preprocess, 21.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1 bus, 3 trucks, 21.8ms\n",
            "Speed: 0.6ms preprocess, 21.8ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1 bus, 2 trucks, 21.4ms\n",
            "Speed: 0.6ms preprocess, 21.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1 bus, 3 trucks, 21.5ms\n",
            "Speed: 0.6ms preprocess, 21.5ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1 bus, 4 trucks, 22.9ms\n",
            "Speed: 0.6ms preprocess, 22.9ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1 bus, 2 trucks, 25.2ms\n",
            "Speed: 0.5ms preprocess, 25.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1 bus, 4 trucks, 25.3ms\n",
            "Speed: 0.5ms preprocess, 25.3ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1 bus, 2 trucks, 22.1ms\n",
            "Speed: 0.6ms preprocess, 22.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1 bus, 2 trucks, 28.8ms\n",
            "Speed: 0.7ms preprocess, 28.8ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1 bus, 2 trucks, 22.3ms\n",
            "Speed: 0.9ms preprocess, 22.3ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1 bus, 2 trucks, 29.0ms\n",
            "Speed: 1.3ms preprocess, 29.0ms inference, 4.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 9 cars, 2 buss, 3 trucks, 24.5ms\n",
            "Speed: 0.6ms preprocess, 24.5ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1 bus, 3 trucks, 23.0ms\n",
            "Speed: 0.6ms preprocess, 23.0ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1 bus, 4 trucks, 1 traffic light, 23.1ms\n",
            "Speed: 0.5ms preprocess, 23.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1 bus, 4 trucks, 1 traffic light, 21.5ms\n",
            "Speed: 0.6ms preprocess, 21.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1 bus, 4 trucks, 22.7ms\n",
            "Speed: 0.6ms preprocess, 22.7ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1 bus, 3 trucks, 21.9ms\n",
            "Speed: 1.0ms preprocess, 21.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1 bus, 4 trucks, 23.7ms\n",
            "Speed: 0.6ms preprocess, 23.7ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1 bus, 4 trucks, 21.9ms\n",
            "Speed: 0.6ms preprocess, 21.9ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1 bus, 5 trucks, 25.4ms\n",
            "Speed: 0.5ms preprocess, 25.4ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1 bus, 5 trucks, 21.4ms\n",
            "Speed: 0.8ms preprocess, 21.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1 bus, 6 trucks, 21.7ms\n",
            "Speed: 1.9ms preprocess, 21.7ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1 bus, 5 trucks, 22.2ms\n",
            "Speed: 0.6ms preprocess, 22.2ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 8 cars, 2 buss, 4 trucks, 29.4ms\n",
            "Speed: 0.6ms preprocess, 29.4ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 9 cars, 2 buss, 3 trucks, 49.8ms\n",
            "Speed: 0.6ms preprocess, 49.8ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1 bus, 5 trucks, 23.0ms\n",
            "Speed: 4.0ms preprocess, 23.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 8 cars, 1 bus, 3 trucks, 22.1ms\n",
            "Speed: 0.8ms preprocess, 22.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1 bus, 3 trucks, 29.9ms\n",
            "Speed: 0.7ms preprocess, 29.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 8 cars, 3 buss, 3 trucks, 22.8ms\n",
            "Speed: 0.6ms preprocess, 22.8ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 8 cars, 2 buss, 1 truck, 22.7ms\n",
            "Speed: 0.6ms preprocess, 22.7ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 9 cars, 1 bus, 3 trucks, 22.6ms\n",
            "Speed: 0.6ms preprocess, 22.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 10 cars, 1 bus, 3 trucks, 22.4ms\n",
            "Speed: 0.7ms preprocess, 22.4ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1 bus, 2 trucks, 22.3ms\n",
            "Speed: 0.6ms preprocess, 22.3ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1 bus, 1 truck, 29.3ms\n",
            "Speed: 0.6ms preprocess, 29.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 11 cars, 2 buss, 2 trucks, 22.9ms\n",
            "Speed: 0.6ms preprocess, 22.9ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 9 cars, 3 buss, 1 truck, 21.6ms\n",
            "Speed: 0.6ms preprocess, 21.6ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 11 cars, 2 buss, 3 trucks, 22.4ms\n",
            "Speed: 0.6ms preprocess, 22.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 10 cars, 2 buss, 3 trucks, 22.4ms\n",
            "Speed: 0.6ms preprocess, 22.4ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 10 cars, 2 buss, 2 trucks, 22.7ms\n",
            "Speed: 0.6ms preprocess, 22.7ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 10 cars, 3 buss, 2 trucks, 21.1ms\n",
            "Speed: 0.8ms preprocess, 21.1ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 10 cars, 2 buss, 4 trucks, 25.9ms\n",
            "Speed: 0.7ms preprocess, 25.9ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 10 cars, 2 buss, 4 trucks, 27.6ms\n",
            "Speed: 0.6ms preprocess, 27.6ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 9 cars, 2 buss, 3 trucks, 22.1ms\n",
            "Speed: 0.6ms preprocess, 22.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 9 cars, 2 buss, 3 trucks, 22.0ms\n",
            "Speed: 0.5ms preprocess, 22.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 9 cars, 2 buss, 3 trucks, 23.4ms\n",
            "Speed: 0.6ms preprocess, 23.4ms inference, 6.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 9 cars, 2 buss, 3 trucks, 24.4ms\n",
            "Speed: 0.6ms preprocess, 24.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 9 cars, 2 buss, 3 trucks, 24.3ms\n",
            "Speed: 0.6ms preprocess, 24.3ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 10 cars, 2 buss, 3 trucks, 1 traffic light, 22.1ms\n",
            "Speed: 0.6ms preprocess, 22.1ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 11 cars, 2 buss, 3 trucks, 1 traffic light, 22.3ms\n",
            "Speed: 0.6ms preprocess, 22.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 10 cars, 2 buss, 3 trucks, 1 traffic light, 23.8ms\n",
            "Speed: 0.5ms preprocess, 23.8ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 11 cars, 1 bus, 3 trucks, 1 traffic light, 23.9ms\n",
            "Speed: 0.6ms preprocess, 23.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 10 cars, 3 buss, 2 trucks, 1 traffic light, 22.9ms\n",
            "Speed: 0.6ms preprocess, 22.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 1 person, 11 cars, 4 buss, 2 trucks, 1 traffic light, 22.4ms\n",
            "Speed: 0.6ms preprocess, 22.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 11 cars, 4 buss, 1 truck, 22.5ms\n",
            "Speed: 0.6ms preprocess, 22.5ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 1 person, 11 cars, 2 buss, 3 trucks, 1 traffic light, 22.6ms\n",
            "Speed: 0.5ms preprocess, 22.6ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 1 person, 9 cars, 2 buss, 3 trucks, 23.3ms\n",
            "Speed: 0.6ms preprocess, 23.3ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 10 cars, 2 buss, 4 trucks, 30.3ms\n",
            "Speed: 1.0ms preprocess, 30.3ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 1 person, 12 cars, 1 bus, 4 trucks, 1 traffic light, 27.6ms\n",
            "Speed: 0.6ms preprocess, 27.6ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 1 person, 11 cars, 3 buss, 4 trucks, 21.2ms\n",
            "Speed: 0.6ms preprocess, 21.2ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 1 person, 9 cars, 1 bus, 3 trucks, 1 traffic light, 26.2ms\n",
            "Speed: 0.7ms preprocess, 26.2ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 1 person, 9 cars, 2 buss, 3 trucks, 1 traffic light, 26.5ms\n",
            "Speed: 0.6ms preprocess, 26.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 1 person, 9 cars, 1 bus, 4 trucks, 1 traffic light, 28.5ms\n",
            "Speed: 0.6ms preprocess, 28.5ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 1 person, 10 cars, 1 bus, 3 trucks, 1 traffic light, 22.9ms\n",
            "Speed: 0.5ms preprocess, 22.9ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 1 person, 10 cars, 1 bus, 3 trucks, 1 traffic light, 25.3ms\n",
            "Speed: 0.6ms preprocess, 25.3ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 1 person, 9 cars, 1 bus, 4 trucks, 1 traffic light, 22.0ms\n",
            "Speed: 0.7ms preprocess, 22.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 1 person, 8 cars, 1 bus, 4 trucks, 1 traffic light, 25.6ms\n",
            "Speed: 0.6ms preprocess, 25.6ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 1 person, 9 cars, 1 bus, 4 trucks, 1 traffic light, 27.1ms\n",
            "Speed: 0.6ms preprocess, 27.1ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 1 person, 8 cars, 2 buss, 2 trucks, 1 traffic light, 31.7ms\n",
            "Speed: 0.9ms preprocess, 31.7ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 1 person, 7 cars, 2 buss, 2 trucks, 1 traffic light, 24.5ms\n",
            "Speed: 0.6ms preprocess, 24.5ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 1 person, 7 cars, 1 bus, 4 trucks, 1 traffic light, 25.7ms\n",
            "Speed: 0.9ms preprocess, 25.7ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 1 person, 7 cars, 3 buss, 3 trucks, 1 traffic light, 26.9ms\n",
            "Speed: 0.7ms preprocess, 26.9ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 1 person, 7 cars, 1 bus, 4 trucks, 1 traffic light, 26.6ms\n",
            "Speed: 0.6ms preprocess, 26.6ms inference, 2.8ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 1 person, 8 cars, 2 buss, 3 trucks, 1 traffic light, 39.8ms\n",
            "Speed: 4.7ms preprocess, 39.8ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 1 person, 7 cars, 2 buss, 2 trucks, 1 traffic light, 39.7ms\n",
            "Speed: 0.6ms preprocess, 39.7ms inference, 4.6ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 1 person, 7 cars, 2 buss, 3 trucks, 1 traffic light, 31.3ms\n",
            "Speed: 0.7ms preprocess, 31.3ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 1 person, 9 cars, 2 buss, 3 trucks, 1 traffic light, 30.5ms\n",
            "Speed: 0.6ms preprocess, 30.5ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 8 cars, 2 buss, 3 trucks, 1 traffic light, 31.7ms\n",
            "Speed: 0.6ms preprocess, 31.7ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 1 person, 6 cars, 2 buss, 5 trucks, 1 traffic light, 27.2ms\n",
            "Speed: 2.9ms preprocess, 27.2ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "0: 384x640 1 person, 7 cars, 1 bus, 4 trucks, 1 traffic light, 27.3ms\n",
            "Speed: 0.6ms preprocess, 27.3ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n"
          ]
        }
      ]
    }
  ]
}